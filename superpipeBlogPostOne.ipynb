{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1: building the pipeline\n",
    "\n",
    "from superpipe.steps import LLMStructuredStep, CustomStep, SERPEnrichmentStep\n",
    "from superpipe import models\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Step 1: use Superpipe's built-in SERP enrichment step to search for the persons wikipedia page\n",
    "# Include a unique \"name\" for the step that will used to reference this step's output in future steps\n",
    "\n",
    "search_step = SERPEnrichmentStep(\n",
    "  prompt= lambda row: f\"{row['name']} wikipedia\",\n",
    "  name=\"search\"\n",
    ")\n",
    "\n",
    "# Step 2: Use an LLM to extract the wikipedia URL from the search results\n",
    "# First, define a Pydantic model that specifies the structured output we want from the LLM\n",
    "\n",
    "class ParseSearchResult(BaseModel):\n",
    "  wikipedia_url: str = Field(description=\"The URL of the Wikipedia page for the person\")\n",
    "\n",
    "# Then we use the built-in LLMStructuredStep and specify a model and a prompt\n",
    "# The prompt is a function that has access to all the fields in the input as well as the outputs of previous steps\n",
    "\n",
    "parse_search_step = LLMStructuredStep(\n",
    "  model=models.gpt35,\n",
    "  prompt= lambda row: f\"Extract the Wikipedia URL for {ro['name']} from the following search results: \\n\\n {row['search']}\",\n",
    "  out_schema=ParseSearchResult,\n",
    "  name=\"parse_search\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpipe.pipeline import Pipeline\n",
    "import requests\n",
    "import html2text\n",
    "import json\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "\n",
    "# Step 3: we create a CustomStep that can execute any arbitrary function (transform)\n",
    "# The function fetches the contents of the wikipedia url and converts them to markdown\n",
    "\n",
    "fetch_wikipedia_step = CustomStep(\n",
    "  transform=lambda row: h.handle(requests.get(row['wikipedia_url']).text),\n",
    "  name=\"wikipedia\"\n",
    ")\n",
    "\n",
    "# Step 4: we extract the date of birth, living/dead status and cause of death from the wikipedia contents\n",
    "\n",
    "class ExtractedData(BaseModel):\n",
    "    date_of_birth: str = Field(description=\"The date of birth of the person in the format YYYY-MM-DD\")\n",
    "    alive: bool = Field(description=\"Whether the person is still alive\")\n",
    "    cause_of_death: str = Field(description=\"The cause of death of the person. If the person is alive, return 'N/A'\")\n",
    "\n",
    "extract_step = LLMStructuredStep(\n",
    "  model=models.gpt4,\n",
    "  prompt= lambda row: f\"\"\"Extract the date of birth for {row['name']}, whether they're still alive \\\n",
    "  and if not, their cause of death from the following Wikipedia content: \\n\\n {row['wikipedia']}\"\"\",\n",
    "  out_schema=ExtractedData,\n",
    "  name=\"extract_data\"\n",
    ")\n",
    "\n",
    "# Finally we define and run the pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "  search_step,\n",
    "  parse_search_step,\n",
    "  fetch_wikipedia_step,\n",
    "  extract_step\n",
    "])\n",
    "\n",
    "output = pipeline.run({\"name\": \"Jean-Paul Sartre\"})\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: evaluating the pipeline\n",
    "\n",
    "broken into two parts:\n",
    "\n",
    "1. **a dataset with labels** - in this case we need a list of famous people and the true date of birth, living status and cause of death of each person\n",
    "2. **evaluation function** - a function that defines what \"correct\" is. We'll use simple comparison for date of birth and living status, and an LLM call to evaluate the correctness of cause of death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "  (\"Ruth Bader Ginsburg\", \"1933-03-15\", False, \"Pancreatic cancer\"),\n",
    "  (\"Bill Gates\", \"1955-10-28\", True, \"N/A\"),\n",
    "  (\"Steph Curry\", \"1988-03-14\", True, \"N/A\"),\n",
    "  (\"Scott Belsky\", \"1980-04-18\", True, \"N/A\"),\n",
    "  (\"Steve Jobs\", \"1955-02-24\", False, \"Pancreatic tumor/cancer\"),\n",
    "  (\"Paris Hilton\", \"1981-02-17\", True, \"N/A\"),\n",
    "  (\"Kurt Vonnegut\", \"1922-11-11\", False, \"Brain injuries\"),\n",
    "  (\"Snoop Dogg\", \"1971-10-20\", True, \"N/A\"),\n",
    "  (\"Kobe Bryant\", \"1978-08-23\", False, \"Helicopter crash\"),\n",
    "  (\"Aaron Swartz\", \"1986-11-08\", False, \"Suicide\")\n",
    "]\n",
    "df = pd.DataFrame([{\"name\": d[0], \"dob_label\": d[1], \"alive_label\": d[2], \"cause_label\": d[3]} for d in data])\n",
    "\n",
    "class EvalResult(BaseModel):\n",
    "  result: bool = Field(description=\"Is the answer correct or not?\")\n",
    "\n",
    "cause_evaluator = LLMStructuredStep(\n",
    "  model=models.gpt4,\n",
    "  prompt=lambda row: f\"This is the correct cause of death: {row['cause_label']}. Is this provided cause of death accurate? The phrasing might be slightly different. Use your judgement: \\n{row['cause_of_death']}\",\n",
    "  out_schema=EvalResult,\n",
    "  name=\"cause_evaluator\")\n",
    "\n",
    "def eval_fn(row):\n",
    "  score = 0\n",
    "  if row['date_of_birth'] == row['dob_label']:\n",
    "    score += 0.25\n",
    "  if row['alive'] == row['alive_label']:\n",
    "    score += 0.25\n",
    "  if row['cause_label'] == \"N/A\":\n",
    "    if row['cause_of_death'] == \"N/A\":\n",
    "      score += 0.5\n",
    "  elif cause_evaluator.run(row)['result']:\n",
    "    score += 0.5  \n",
    "  return score\n",
    "\n",
    "pipeline.run(df)\n",
    "print(\"Score: \", pipeline.evaluate(eval_fn))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: optimizing the pipeline\n",
    "\n",
    "this pipeline has an accuracy score of 100%, but perhaps there's room for improvement on cost and speed. First let's view the cost and latency of each step to figure out which one is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in pipeline.steps:\n",
    "  print(f\"Step {step.name}:\")\n",
    "  print(f\"- Latency: {step.statistics.total_latency}\")\n",
    "  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the final step (`extract_data`) is the one responsible for the bulk of the cost and latency. This makes sense, because we're feeding in the entire wikipedia article to GPT-4, one of the most expensive models.\n",
    "\n",
    "Let's find out if we can get away with a cheaper/faster model. Most models cannot handle the number of tokens needed to ingest a whole wikipedia article, so we'll turn to the two that can that are also cheaper than GPT4: Claude 3 Sonnet and Claude 3 Haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpipe.grid_search import GridSearch\n",
    "from superpipe.models import claude3_haiku, claude3_sonnet\n",
    "from superpipe.steps import LLMStructuredCompositeStep\n",
    "\n",
    "# we need to use LLMStructuredCompositeStep which uses GPT3.5 for structured JSON extraction\n",
    "# because Claude does not support JSON mode or function calling out of the box\n",
    "new_extract_step = LLMStructuredCompositeStep(\n",
    "  model=models.claude3_haiku,\n",
    "  prompt=extract_step.prompt,\n",
    "  out_schema=ExtractedData,\n",
    "  name=\"extract_data_new\"\n",
    ")\n",
    "\n",
    "new_pipeline = Pipeline([\n",
    "  search_step,\n",
    "  parse_search_step,\n",
    "  fetch_wikipedia_step,\n",
    "  new_extract_step\n",
    "], evaluation_fn=eval_fn)\n",
    "\n",
    "param_grid = {\n",
    "  new_extract_step.name:{\n",
    "    \"model\": [claude3_haiku, claude3_sonnet]}\n",
    "}\n",
    "grid_search = GridSearch(new_pipeline, param_grid)\n",
    "grid_search.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, Claude 3 Haiku is both more accurate (100% v/s 45%) as well as cheaper and faster. This is suprising, but useful information that we wouldn't have found out unless we built and evaluated pipelines on _our specific data_ rather than benchmark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params\n",
    "new_pipeline.update_params(best_params)\n",
    "new_pipeline.run(df)\n",
    "print(\"Score: \", new_pipeline.score)\n",
    "for step in new_pipeline.steps:\n",
    "  print(f\"Step {step.name}:\")\n",
    "  print(f\"- Latency: {step.statistics.total_latency}\")\n",
    "  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
