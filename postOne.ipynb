{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1: building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpipe.steps import LLMStructuredStep, CustomStep, SERPEnrichmentStep\n",
    "from superpipe.clients import init_openai\n",
    "from superpipe import models\n",
    "from pydantic import BaseModel, Field, HttpUrl\n",
    "import os\n",
    "\n",
    "# get the openai api] key from the environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# initialize the opeani client with the API key\n",
    "if openai_api_key is not None:\n",
    "    init_openai(openai_api_key)\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY environment variable not set\")\n",
    "\n",
    "# get the api key from the environment variable, or raise an error if it doesn't exist\n",
    "api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"SERPAPI_API_KEY environment variable not set\")\n",
    "\n",
    "# step 1: use superpipe's built-in serp enrichment step to search for the person's wikipedia page\n",
    "# include a unique \"name\" for the step that will be used to reference this step's output in future steps\n",
    "search_step = SERPEnrichmentStep(\n",
    "    prompt=lambda row: f\"{row['name']} wikipedia\",\n",
    "    name=\"search\"\n",
    ")\n",
    "\n",
    "# step 2: use an llm to extract the wikipedia url from the search results\n",
    "# first, define a pydantic model that specifies the structured output we want from the llm\n",
    "class ParseSearchResult(BaseModel):\n",
    "    wikipedia_url: HttpUrl  # ensures that the extracted url is valid\n",
    "\n",
    "# adjust the prompt to clearly ask for the wikipedia url extraction\n",
    "parse_search_step = LLMStructuredStep(\n",
    "    model=models.gpt4,\n",
    "    prompt=lambda row: (\n",
    "        f\"Extract the Wikipedia URL for {row['name']} from the following search results: \\n\\n\"\n",
    "        f\"{row['search']}\\n\\n\"\n",
    "        \"Provide the URL in a clear and concise format.\"\n",
    "    ),\n",
    "    out_schema=ParseSearchResult,\n",
    "    name=\"parse_search\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpipe.pipeline import Pipeline\n",
    "import requests\n",
    "import html2text\n",
    "import json\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "\n",
    "# step 3: we create a customstep that can execute any arbitrary function (transform)\n",
    "# the function fetches the contents of the wikipedia url and converts them to markdown\n",
    "fetch_wikipedia_step = CustomStep(\n",
    "  transform=lambda row: h.handle(requests.get(row['wikipedia_url']).text),\n",
    "  name=\"wikipedia\"  \n",
    ")\n",
    "\n",
    "# step 4: we extract the date of birth, living/dead status and cause of death from the wikipedia contents\n",
    "class ExtractedData(BaseModel):\n",
    "    date_of_birth: str = Field(description=\"The date of birth of the person in the format YYYY-MM-DD\")\n",
    "    alive: bool = Field(description=\"Whether the person is still alive\")\n",
    "    cause_of_death: str = Field(description=\"The cause of death of the person. If the person is alive, return 'N/A'\")\n",
    "\n",
    "extract_step = LLMStructuredStep(\n",
    "  model=models.gpt4,\n",
    "  prompt= lambda row: f\"\"\"Extract the date of birth for {row['name']}, whether they're still alive \\\n",
    "  and if not, their cause of death from the following Wikipedia content: \\n\\n {row['wikipedia']}\"\"\",\n",
    "  out_schema=ExtractedData,\n",
    "  name=\"extract_data\"\n",
    ")\n",
    "\n",
    "# finally we define and run the pipeline\n",
    "pipeline = Pipeline([\n",
    "  search_step,\n",
    "  parse_search_step,\n",
    "  fetch_wikipedia_step,\n",
    "  extract_step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2: evaluating the pipeline\n",
    "\n",
    "broken into the folliowing parts:\n",
    "\n",
    "1. **a dataset with labels** - in this case we need a list of famous people and the true date of birth, living status and cause of death of each person\n",
    "2. **evaluation function** - a function that defines what \"correct\" is. We'll use simple comparison for date of birth and living status, and an LLM call to evaluate the correctness of cause of death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    " ('Ruth Bader Ginsburg', '1933-03-15', False, 'Pancreatic cancer'),\n",
    " ('Bill Gates', '1955-10-28', True, 'N/A'),\n",
    " ('Steph Curry', '1988-03-14', True, 'N/A'),\n",
    " ('Scott Belsky', '1980-04-18', True, 'N/A'),\n",
    " ('Steve Jobs', '1955-02-24', False, 'Respiratory Arrest Related to a Neuroendocrine Tumor'),\n",
    " ('Paris Hilton', '1981-02-17', True, 'N/A'),\n",
    " ('Kurt Vonnegut', '1922-11-11', False, 'Brain injuries'),\n",
    " ('Snoop Dogg', '1971-10-20', True, 'N/A'),\n",
    " ('Kobe Bryant', '1978-08-23', False, 'Helicopter crash'),\n",
    " ('Aaron Swartz', '1986-11-08', False, 'Suicide'),\n",
    " ('Albert Einstein', '1879-03-14', False, 'Aortic Aneurysm'),\n",
    " ('Martin Luther King Jr.', '1929-01-15', False, 'Assassination By Firearm'),\n",
    " ('Marilyn Monroe', '1926-06-01', False, 'Drug Overdose'),\n",
    " ('Leonardo da Vinci', '1452-04-15', False, 'Stroke'),\n",
    " ('William Shakespeare', '1564-04-26', False, 'Natural Causes'),\n",
    " ('Frida Kahlo', '1907-07-06', False, 'Pulmonary Embolism'),\n",
    " ('Vincent van Gogh', '1853-04-30', False, 'Suicide by Gunshot'),\n",
    " ('Isaac Newton', '1643-01-04', False, 'Natural Causes'),\n",
    " ('Pablo Picasso', '1881-10-25', False, 'Pulmonary Edema & Heart Attack'),\n",
    " ('Mahatma Gandhi', '1869-10-02', False, 'Assassination By Firearm'),\n",
    " ('Jane Austen', '1775-12-16', False, \"Addison'S Disease\"),\n",
    " ('Charles Darwin', '1809-02-12', False, 'Coronary Thrombosis'),\n",
    " ('Wolfgang Amadeus Mozart', '1756-01-27', False, 'Unknown'),\n",
    " ('Princess Diana', '1961-07-01', False, 'Car Crash'),\n",
    " ('Nelson Mandela', '1918-07-18', False, 'Respiratory Infection'),\n",
    " ('Bruce Lee', '1940-11-27', False, 'Cerebral Edema'),\n",
    " ('Sigmund Freud', '1856-05-06', False, 'Euthanasia'),\n",
    " ('Amelia Earhart', '1897-07-24', False, 'Presumed Dead After Disappearance over the Pacific Ocean.'),\n",
    " ('Malcolm X', '1925-05-19', False, 'Assassination'),\n",
    " ('Anne Frank', '1929-06-12', False, 'Typhus'),\n",
    " ('Galileo Galilei', '1564-02-15', False, 'Fever & Heart Palpitations'),\n",
    " ('Charlie Chaplin', '1889-04-16', False, 'Stroke'),\n",
    " ('Elvis Presley', '1935-01-08', False, 'Cardiac Arrest '),\n",
    " ('Michael Jackson', '1958-08-29', False, 'Propofol Overdose'),\n",
    " ('Nikola Tesla', '1856-07-10', False, 'Coronary Thrombosis'),\n",
    " ('Florence Nightingale', '1820-05-12', False, 'Natural Causes'),\n",
    " ('Edgar Allan Poe', '1809-01-19', False, 'Unknown'),\n",
    " ('Marie Curie', '1867-11-07', False, 'Aplastic Anemia'),\n",
    " ('Abraham Lincoln', '1809-02-12', False, 'Assassination'),\n",
    " ('George Washington', '1732-02-22', False, 'Epiglottitis'),\n",
    " ('Ada Lovelace', '1815-12-10', False, 'Uterine Cancer'),\n",
    " ('James Dean', '1931-02-08', False, 'Car Crash'),\n",
    " ('Tupac Shakur', '1971-06-16', False, 'Murder By Firearm'),\n",
    " ('Stepehen Hawking', '1942-01-08', False, 'Natural Causes'),\n",
    " ('Elon Musk', '1971-06-28', True, 'N/A'),\n",
    " ('John Doerr', '1951-06-29', True, 'N/A'),\n",
    " ('Harry Stebbings', '1996-06-22', True, 'N/A'),\n",
    " ('Cory Booker', '1969-04-27', True, 'N/A'),\n",
    " ('Noah Kahan', '1997-01-01', True, 'N/A'),\n",
    " ('Sam Altman', '1985-04-22', True, 'N/A')\n",
    "]\n",
    "df = pd.DataFrame([{\"name\": d[0], \"dob_label\": d[1], \"alive_label\": d[2], \"cause_label\": d[3]} for d in data])\n",
    "\n",
    "class EvalResult(BaseModel):\n",
    "  result: bool = Field(description=\"Is the answer correct or not?\")\n",
    "\n",
    "cause_evaluator = LLMStructuredStep(\n",
    "  model=models.gpt4,\n",
    "  prompt=lambda row: f\"This is the correct cause of death: {row['cause_label']}. Is this provided cause of death accurate? The phrasing might be slightly different. Use your judgement: \\n{row['cause_of_death']}\",\n",
    "  out_schema=EvalResult,\n",
    "  name=\"cause_evaluator\")\n",
    "\n",
    "def eval_fn(row):\n",
    "  score = 0\n",
    "  if row['date_of_birth'] == row['dob_label']:\n",
    "    score += 0.25\n",
    "  if row['alive'] == row['alive_label']:\n",
    "    score += 0.25\n",
    "  if row['cause_label'] == \"N/A\":\n",
    "    if row['cause_of_death'] == \"N/A\":\n",
    "      score += 0.5\n",
    "  elif cause_evaluator.run(row)['result']:\n",
    "    score += 0.5  \n",
    "  return score\n",
    "\n",
    "pipeline.run(df)\n",
    "print(\"Score: \", pipeline.evaluate(eval_fn))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3: optimizing the pipeline\n",
    "\n",
    "this pipeline has an accuracy score of 86.5%, but perhaps there's room for improvement on cost and speed. First let's view the cost and latency of each step to figure out which one is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step search:\n",
      "- Latency: 4.191360712051392\n",
      "- Cost: 0.0\n",
      "Step parse_search:\n",
      "- Latency: 4.446325675999105\n",
      "- Cost: 0.0033809999999999995\n",
      "Step wikipedia:\n",
      "- Latency: 1.655771255493164\n",
      "- Cost: 0.0\n",
      "Step extract_data:\n",
      "- Latency: 29.421274683001684\n",
      "- Cost: 1.90233\n"
     ]
    }
   ],
   "source": [
    "for step in pipeline.steps:\n",
    "  print(f\"Step {step.name}:\")\n",
    "  print(f\"- Latency: {step.statistics.total_latency}\")\n",
    "  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the final step (`extract_data`) is the one responsible for the bulk of the cost and latency. This makes sense, because we're feeding in the entire wikipedia article to GPT-4, one of the most expensive models.\n",
    "\n",
    "Let's find out if we can get away with a cheaper/faster model. Most models cannot handle the number of tokens needed to ingest a whole wikipedia article, so we'll turn to the two that can that are also cheaper than GPT4: Claude 3 Sonnet and Claude 3 Haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying step search: 100%|██████████| 3/3 [00:02<00:00,  1.16it/s]\n",
      "Applying step parse_search: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\n",
      "Applying step wikipedia: 100%|██████████| 3/3 [00:01<00:00,  2.38it/s]\n",
      "Applying step extract_data_new: 100%|██████████| 3/3 [00:00<00:00,  4.26it/s]\n",
      "Applying step search: 100%|██████████| 3/3 [00:02<00:00,  1.09it/s]\n",
      "Applying step parse_search: 100%|██████████| 3/3 [00:04<00:00,  1.37s/it]\n",
      "Applying step wikipedia: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]\n",
      "Applying step extract_data_new: 100%|██████████| 3/3 [00:00<00:00,  4.14it/s]\n",
      "Applying step search: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
      "Applying step parse_search: 100%|██████████| 3/3 [00:02<00:00,  1.32it/s]\n",
      "Applying step wikipedia: 100%|██████████| 3/3 [00:01<00:00,  2.36it/s]\n",
      "Applying step extract_data_new: 100%|██████████| 3/3 [00:00<00:00,  4.02it/s]\n",
      "/home/ccasey/Downloads/superpipe/.venv/lib/python3.10/site-packages/superpipe/util.py:44: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styler = styler.applymap(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d3acd_row0_col1, #T_d3acd_row1_col1, #T_d3acd_row1_col4, #T_d3acd_row2_col1, #T_d3acd_row2_col2 {\n",
       "  background-color: rgb(255,255,0);\n",
       "  color: black;\n",
       "}\n",
       "#T_d3acd_row0_col2, #T_d3acd_row2_col4 {\n",
       "  background-color: rgb(0,255,0);\n",
       "  color: black;\n",
       "}\n",
       "#T_d3acd_row0_col3, #T_d3acd_row1_col3, #T_d3acd_row2_col3 {\n",
       "  background-color: rgb(0,255,255);\n",
       "  color: black;\n",
       "}\n",
       "#T_d3acd_row0_col4, #T_d3acd_row1_col2 {\n",
       "  background-color: rgb(255,0,0);\n",
       "  color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d3acd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d3acd_level0_col0\" class=\"col_heading level0 col0\" >extract_data_new__model</th>\n",
       "      <th id=\"T_d3acd_level0_col1\" class=\"col_heading level0 col1\" >score</th>\n",
       "      <th id=\"T_d3acd_level0_col2\" class=\"col_heading level0 col2\" >input_cost</th>\n",
       "      <th id=\"T_d3acd_level0_col3\" class=\"col_heading level0 col3\" >output_cost</th>\n",
       "      <th id=\"T_d3acd_level0_col4\" class=\"col_heading level0 col4\" >total_latency</th>\n",
       "      <th id=\"T_d3acd_level0_col5\" class=\"col_heading level0 col5\" >input_tokens</th>\n",
       "      <th id=\"T_d3acd_level0_col6\" class=\"col_heading level0 col6\" >output_tokens</th>\n",
       "      <th id=\"T_d3acd_level0_col7\" class=\"col_heading level0 col7\" >num_success</th>\n",
       "      <th id=\"T_d3acd_level0_col8\" class=\"col_heading level0 col8\" >num_failure</th>\n",
       "      <th id=\"T_d3acd_level0_col9\" class=\"col_heading level0 col9\" >index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d3acd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d3acd_row0_col0\" class=\"data row0 col0\" >claude-3-haiku-20240307</td>\n",
       "      <td id=\"T_d3acd_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_d3acd_row0_col2\" class=\"data row0 col2\" >0.002271</td>\n",
       "      <td id=\"T_d3acd_row0_col3\" class=\"data row0 col3\" >0.000094</td>\n",
       "      <td id=\"T_d3acd_row0_col4\" class=\"data row0 col4\" >9.336668</td>\n",
       "      <td id=\"T_d3acd_row0_col5\" class=\"data row0 col5\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 4542, 'claude-3-haiku-20240307': 0})</td>\n",
       "      <td id=\"T_d3acd_row0_col6\" class=\"data row0 col6\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 63, 'claude-3-haiku-20240307': 0})</td>\n",
       "      <td id=\"T_d3acd_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_d3acd_row0_col8\" class=\"data row0 col8\" >3</td>\n",
       "      <td id=\"T_d3acd_row0_col9\" class=\"data row0 col9\" >3791042982912507671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3acd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d3acd_row1_col0\" class=\"data row1 col0\" >claude-3-sonnet-20240229</td>\n",
       "      <td id=\"T_d3acd_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_d3acd_row1_col2\" class=\"data row1 col2\" >0.002474</td>\n",
       "      <td id=\"T_d3acd_row1_col3\" class=\"data row1 col3\" >0.000094</td>\n",
       "      <td id=\"T_d3acd_row1_col4\" class=\"data row1 col4\" >8.099901</td>\n",
       "      <td id=\"T_d3acd_row1_col5\" class=\"data row1 col5\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 4948, 'claude-3-sonnet-20240229': 0})</td>\n",
       "      <td id=\"T_d3acd_row1_col6\" class=\"data row1 col6\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 63, 'claude-3-sonnet-20240229': 0})</td>\n",
       "      <td id=\"T_d3acd_row1_col7\" class=\"data row1 col7\" >0</td>\n",
       "      <td id=\"T_d3acd_row1_col8\" class=\"data row1 col8\" >3</td>\n",
       "      <td id=\"T_d3acd_row1_col9\" class=\"data row1 col9\" >3303757701480512865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3acd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d3acd_row2_col0\" class=\"data row2 col0\" >claude-3-opus-20240229</td>\n",
       "      <td id=\"T_d3acd_row2_col1\" class=\"data row2 col1\" >1.000000</td>\n",
       "      <td id=\"T_d3acd_row2_col2\" class=\"data row2 col2\" >0.002419</td>\n",
       "      <td id=\"T_d3acd_row2_col3\" class=\"data row2 col3\" >0.000094</td>\n",
       "      <td id=\"T_d3acd_row2_col4\" class=\"data row2 col4\" >5.857355</td>\n",
       "      <td id=\"T_d3acd_row2_col5\" class=\"data row2 col5\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 4839, 'claude-3-opus-20240229': 0})</td>\n",
       "      <td id=\"T_d3acd_row2_col6\" class=\"data row2 col6\" >defaultdict(<class 'int'>, {'gpt-3.5-turbo-0125': 63, 'claude-3-opus-20240229': 0})</td>\n",
       "      <td id=\"T_d3acd_row2_col7\" class=\"data row2 col7\" >0</td>\n",
       "      <td id=\"T_d3acd_row2_col8\" class=\"data row2 col8\" >3</td>\n",
       "      <td id=\"T_d3acd_row2_col9\" class=\"data row2 col9\" >-8345258512936141949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x771e801b5ab0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from superpipe.grid_search import GridSearch\n",
    "from superpipe.clients import init_anthropic\n",
    "from superpipe.models import claude3_haiku, claude3_sonnet, claude3_opus\n",
    "from superpipe.steps import LLMStructuredCompositeStep\n",
    "import os\n",
    "\n",
    "# get the claude API key from the environment variable\n",
    "anthropic_api_key = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "\n",
    "# Initialize the anthropic client with the API key\n",
    "if anthropic_api_key is not None:\n",
    "    init_anthropic(openai_api_key)\n",
    "else:\n",
    "    print(\"ANTHROPIC_API_KEY environment variable not set\")\n",
    "\n",
    "# we need to use LLMStructuredCompositeStep which uses GPT3.5 for structured JSON extraction\n",
    "# because Claude does not support JSON mode or function calling out of the box\n",
    "new_extract_step = LLMStructuredCompositeStep(\n",
    "  model=models.claude3_haiku,\n",
    "  prompt=extract_step.prompt,\n",
    "  out_schema=ExtractedData,\n",
    "  name=\"extract_data_new\"\n",
    ")\n",
    "\n",
    "new_pipeline = Pipeline([\n",
    "  search_step,\n",
    "  parse_search_step,\n",
    "  fetch_wikipedia_step,\n",
    "  new_extract_step\n",
    "], evaluation_fn=eval_fn)\n",
    "\n",
    "param_grid = {\n",
    "  new_extract_step.name:{\n",
    "    \"model\": [claude3_haiku, claude3_sonnet, claude3_opus]}\n",
    "}\n",
    "grid_search = GridSearch(new_pipeline, param_grid)\n",
    "grid_search.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely, Claude 3 Haiku is both more accurate (100% v/s 45%) as well as cheaper and faster. This is suprising, but useful information that we wouldn't have found out unless we built and evaluated pipelines on _our specific data_ rather than benchmark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying step search: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n",
      "Applying step parse_search: 100%|██████████| 3/3 [00:03<00:00,  1.29s/it]\n",
      "Applying step wikipedia: 100%|██████████| 3/3 [00:01<00:00,  2.32it/s]\n",
      "Applying step extract_data_new: 100%|██████████| 3/3 [00:00<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  1.0\n",
      "Step search:\n",
      "- Latency: 2.5014584064483643\n",
      "- Cost: 0.0\n",
      "Step parse_search:\n",
      "- Latency: 3.8747405909998633\n",
      "- Cost: 0.002477\n",
      "Step wikipedia:\n",
      "- Latency: 1.2917284965515137\n",
      "- Cost: 0.0\n",
      "Step extract_data_new:\n",
      "- Latency: 0.0\n",
      "- Cost: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params\n",
    "new_pipeline.update_params(best_params)\n",
    "new_pipeline.run(df)\n",
    "print(\"Score: \", new_pipeline.score)\n",
    "for step in new_pipeline.steps:\n",
    "  print(f\"Step {step.name}:\")\n",
    "  print(f\"- Latency: {step.statistics.total_latency}\")\n",
    "  print(f\"- Cost: {step.statistics.input_cost + step.statistics.output_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
